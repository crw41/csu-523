---
title: "Lab 5: CAMELS Data"
subtitle: "Ecosystem Science and Sustainability 523C"
author:
  name: "Charlotte Wachter"
  email: "crw41@colostate.edu"
format: 
  html: 
    code-fold: true
    toc: true
---

```{r}
#| label: load-packages
#| include: false

# Libraries
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)

# Visualization
library(flextable)
library(gghighlight)
library(ggrepel)
library(ggthemes)
library(knitr)
library(patchwork)

knitr::opts_chunk$set(fig.width = 6, 
                      message = FALSE, 
                      warning = FALSE, 
                      comment = "", 
                      cache = FALSE, 
                      fig.retina = 3)
```

# **Question 1:** Download Data
```{r}
# Data download
root <- 'https://gdex.ucar.edu/dataset/camels/file'

# PDF
download.file(url = paste0(root, '/camels_attributes_v2.0.pdf'), 
              destfile = 'lab-05-data/camels_attributes_v2.0.pdf')

# Basin characteristics
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('lab-05-data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE)
camels <- power_full_join(camels ,by = 'gauge_id')
```

All CAMELS data and the PDF are downloaded in my data directory! According to the documentation PDF, the variable "zero_q_freq" is the frequency of days with daily discharge (Q) = 0 mm/day. 

## ***Exploratory Data Analysis***
```{r}
# Mapping sites
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map() +
  coord_fixed(1.3)
```

# **Question 2:** Make 2 Maps
```{r}
# Aridity
aridity_map <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "dodgerblue", high = "darkorange", name = "Aridity") +
  labs(title = "Guage Sites by Aridity") +
  ggthemes::theme_map() +
  theme(plot.title = element_text(size = 12, face = "bold",color = "black"),
        legend.position = "bottom",
        legend.direction = "horizontal") +
  coord_fixed(1.3)

# Mean daily precip
precip_map <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "lightblue", high = "darkblue", name = "Mean Daily Precip. (mm/day)") +
  labs(title = "Guage Sites by Precipitation") +
  ggthemes::theme_map() +
  theme(plot.title = element_text(size = 12, face = "bold",color = "black"),
        legend.position = "bottom",
        legend.direction = "horizontal") +
  coord_fixed(1.3)


aridity_map + precip_map
```

# **Question 3:** Build a xgboost and neural network model

## ***Preparation*** - building recipe and workflow, initial lm and rf models
```{r}
# Set seed
set.seed(123)

# Transforming outcome var
camels <- camels %>%
  mutate(logQmean = log(q_mean))

# Splitting data
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)
camels_cv <- vfold_cv(camels_train, v = 10)

# Recipe to preprocess data
rec <- recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform predictor vars
  step_log(all_predictors()) %>%
  # Add interaction term btw aridity and p_mean
  step_interact(terms = ~aridity:p_mean) %>%
  # Drop any rows with missing values
  step_naomit(all_predictors(), all_outcomes())
  
# Define model
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Workflow
lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train)

# Extract coefficients from workflow 
summary(extract_fit_engine(lm_wf))$coefficients 

# Making predictions 
lm_data <- augment(lm_wf, new_data = camels_test)

# Using random forest instead
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train)

rf_data <- augment(rf_wf, new_data = camels_test)

# Easier comparison with workflow_set
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

autoplot(wf)
```

## ***Building models, adding to workflow, evaluating and comparing***
```{r}
# Gradient boost (boosted trees)
bt_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Neural network model
nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

# Adding to workflow
wf <- workflow_set(list(rec), 
                   list(lm_model, rf_model, bt_model, nn_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

# Comparison
autoplot(wf)
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

The output from the autoplot and rank_results functions show that the neural network outperforms the random forest, linear, and boosted trees models. Based on these comparisons, I would move forward with the neural network model. 

# **Question 4a:** Data Prep / Data Splitting

```{r}
# Set seed
set.seed(123)

# Transforming outcome var (already done above in earlier chunks, but for completeness),
# also subsetting to variables I'm interested in
camels2 <- camels %>%
  select(q_mean, p_mean, aridity, high_prec_freq, high_prec_dur, low_prec_freq,
         low_prec_dur, frac_forest, lai_max, gvf_max, slope_mean, elev_mean,
         soil_porosity, soil_conductivity, max_water_content, geol_porostiy,
         geol_permeability, area_gages2) %>%
  mutate(logQmean = log(q_mean)) %>%
  select(-q_mean) # Have to drop it for recipe later

# Splitting data with 75% used for training, 25% for testing
camels_split2 <- initial_split(camels2, prop = 0.75)
camels_train2 <- training(camels_split2)
camels_test2 <- testing(camels_split2)
camels_cv2 <- vfold_cv(camels_train2, v = 10)
```

# **Question 4b:** Recipe

I have never predicted streamflow before, so I will start with a large subset of variables from the CAMELS dataset. I will include variables related to climate (p_mean, aridity, high_prec_freq, high_prec_dur, low_prec_freq, low_prec_dur), land cover (frac_forest, lai_max, gvf_max), topography (slope_mean, elev_mean), soil and geology (soil_porosity, soil_conductivity, max_water_content, geol_porosity, geol_permeability), and, lastly, catchement area (area_gages2). I will apply a log transformation to all rainfall related variables (since we know they are right-skewed), normalize all variables (standarizing), and remove highly correlated predictors as well as near-constant predictors (since they not contributing much information). I chose not to interact the forest cover variable with mean daily rainfall although it's possible that forest cover moderates how high precipitation affects streamflow. This decision is because I plan on using a randfom forest, gradient boosting (boosted trees), and a neural network. The tree-based models implicitly model interactions and the neural network can learn interactions on its own, so I don't need to include them in the recipe. 

```{r}
# Recipe to preprocess data
rec2 <- recipe(logQmean ~ ., data = camels_train2) %>%
  # Log transform rainfall vars
  step_log(p_mean, high_prec_freq, high_prec_dur, low_prec_freq, low_prec_dur) %>%
  # Normalize predictors
  step_normalize(all_predictors()) %>%
  # Remove predictors that are highly correlated 
  step_corr(all_predictors(), threshold = 0.9) %>%
  # Remove near-constant predictors
  step_nzv(all_predictors()) %>%
  # Drop any rows with missing values
  step_naomit(all_predictors(), all_outcomes())
```

# **Question 4c:** Define 3 Models
```{r}
# Random Forest
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# Boosted trees
bt_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Neural network
nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
```

# **Question 4d:** Workflow Set
```{r}
# Adding to workflow
wf2 <- workflow_set(list(rec2), 
                    list(rf_model, bt_model, nn_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv2)
```

# **Question 4e:** Evaluation
```{r}
# Comparison
autoplot(wf2)
rank_results(wf2, rank_metric = "rsq", select_best = TRUE)
```
Based on the output from autoplot and rank_results, the neural network is the best model. It is the only model with a R-squared value > 0.9, so, in this sense, it is the only "successful" model. Still, the random forest model is close to having a R-squared value > 0.90 and might get there with tuning the model hyperparameters, so I will continue with this model because it will allow me to look at variable importance later on. 

## ***Tuning the RF Model***
```{r}
# Tunable model (want to tune the number of variables at each split and the minimum number of data points in a node)
rf_tune_model <- rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

# Tuning grid
rf_tune_grid <- grid_regular(
  mtry(range = c(2, 10)), 
  min_n(range = c(2, 10)), 
  levels = 5)

# Workflow
rf_tune_wf <- workflow() %>%
  add_model(rf_tune_model) %>%
  add_recipe(rec2)

# Tuning!
rf_tuned <- tune_grid(rf_tune_wf,
                      resamples = camels_cv2,
                      grid = rf_tune_grid,
                      metric_set(rmse, rsq))

# Looking at best results
show_best(rf_tuned, metric = "rsq", n = 5)

# Finalizing workflow with best parameters
best_rf <- select_best(rf_tuned, metric = "rsq")
final_wf <- finalize_workflow(rf_tune_wf, best_rf)

# Fitting to data
final_fit <- fit(final_wf, data = camels_train2)
```

# **Question 4g:** Look at VIP
```{r}
# Extracting fitted model
fitted_model <- extract_fit_parsnip(final_fit)

# Plotting variable importance
vip(fitted_model)
```

The variable importance plot shows that the most important predictors of log mean flow are aridity, mean daily precipitation, the frequency of low precipitation events, and forest cover, although forest cover is noticeably less influential than the top three most important variables. These results make sense to me and are consistent with hydrological theory, which highlights the importance of rainfall and evapotranspiration (modeled here by aridity and other catchement terrain characteristics, like forest cover) in predicting flows. I was a little surprised that the variables capturing the frequency and duration of high precipitation events didn't have more predictive power, but I can see how the frequency of low precipitation days contributes to the model by introducing information about the intermittent nature of precipitation.

# **Question 4f:** Extract and Evaluate
```{r}
# Adding predictions
test_predictions <- augment(final_fit, new_data = camels_test2)

# Evaluate performance
metrics(test_predictions, truth = logQmean, estimate = .pred)

# Plotting
ggplot(test_predictions, aes(x = logQmean, y = .pred)) +
  geom_point(aes(color = .pred), alpha = 0.7) +
  geom_abline() +
  geom_smooth(method = "lm", col = "red", lty = 2, se = FALSE) +
  scale_color_viridis_c(option = "C", name = "Predicted\nLog Mean\nFlow") +
  labs(title = "Tuned Random Forest Model: Observed vs Predicted Log Mean Flow",
       x = "Observed Log Mean Flow)",
       y = "Predicted Log Mean Flow)") +
  theme_minimal()
```

Given that log mean flow ranges from 2 to -4 with a standard deviation of 1.31, the RMSE of the model (~0.4) is relatively low. Also, the R-squared value (~0.92) has improved a lot compared to the untuned model. Together, the RMSE and R-squared value suggest that the model fits the data pretty well. The plot confirms that the model predicts the data well overall, but illustrates that the model performs worse when predicting lower values of log mean flow as points are more disperse and the model trend line (dashed red) deviates further from the 45 degree line in the lower left corner of the plot. 
